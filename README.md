# Sentiment_Analysis
ğŸŒ³NIFoS Datascience

Korean Text Multi-Classification Process
### Set up

    git clone git@github.com:DDalaDDula/NIFoS_Sentiment_Analysis.git
    cd NIFoS_Sentiment_Analysis

---
### Environment

    conda env create -f NIFOS.yaml
    conda activate NIFOS

---
**ELECTRA**Â (**E**fficientlyÂ **L**earning anÂ **E**ncoder thatÂ **C**lassifiesÂ **T**okenÂ **R**eplacementsÂ **A**ccurately)ë€ ICLR(**I**nternational **C**onference on **L**earning **R**epresentations) 2020ì—ì„œ GOOGLE RESEARCH íŒ€ì´ ë°œí‘œí•œ Language Model ì…ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì€ ìƒˆë¡œìš´ pre-training(ì‚¬ì „í•™ìŠµ) ê¸°ë²•ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

GAN (Generative Adversarial Network) ê¸°ë°˜ì˜ ëª¨ë¸ë¡œ, ëŒ€ì²´ëœ í† í°ì„ ê°ì§€í•˜ëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì…ë‹ˆë‹¤.

**ELECTRA**ëª¨ë¸ì€ í•™ìŠµ íš¨ìœ¨ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´Â **R**eplaced **T**oken **D**etection (**RTD**)ì´ë¼ëŠ” ìƒˆë¡œìš´ pre-training Taskë¥¼ ì œì•ˆí•˜ë©° ë³´ë‹¤ ë¹ ë¥´ê³  íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ELECTRA**ëª¨ë¸ì€ ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì˜ í¬ê¸°, ë°ì´í„°, ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ë™ì¼í•œ ì¡°ê±´ì—ì„œÂ ê¸°ì¡´ [**GOOGLE BERT**](https://github.com/google-research/bert) ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. 

ë‹¤ì¤‘ë¶„ë¥˜ì— ì‚¬ìš©ëœ ëª¨ë¸ì€ monologgë‹˜ì˜ [**koELECTRA**](https://github.com/monologg/KoELECTRA/tree/master)ëª¨ë¸ì…ë‹ˆë‹¤. **koELECTRA**ëŠ” ELECTRA ëª¨ë¸ì„ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡  í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼  ëŒ€ê·œëª¨ë¡œ pre-training(ì‚¬ì „í•™ìŠµ)ì‹œí‚¨ ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ë²ˆ 3ì§„ ë¶„ë¥˜ì— í™œìš©ëœ ëª¨ë¸ì€ koELECTRAì˜ [**koelectra-base-v3-discriminator**](https://huggingface.co/monologg/koelectra-base-v3-discriminator)ëª¨ë¸ë¡œ, 34GBì˜ í•œêµ­ì–´ Corpus(ë‰´ìŠ¤, ìœ„í‚¤, ë‚˜ë¬´ìœ„í‚¤, ì‹ ë¬¸, ë¬¸ì–´, êµ¬ì–´, ë©”ì‹ ì €, ì›¹)ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤.

### **About KoELECTRA**

|  | Layers | Embedding Size | Hidden Size | # heads |
| --- | --- | --- | --- | --- |
| KoELECTRA-Base | Discriminator | 12 | 768 | 768 |
|  | Generator | 12 | 768 | 256 |

### **Pretraining Details**

| Model | Batch Size | Train Steps | LR | Max Seq Len | Generator Size | Train Time |
| --- | --- | --- | --- | --- | --- | --- |
| Base v3 | 256 | 1.5M | 2e-4 | 512 | 0.33 | 14d |

`Batch size`ì™€Â `Train steps`ì„ ì œì™¸í•˜ê³ ëŠ”Â ì› ë…¼ë¬¸ì˜ **Hyperparameter**ì™€ ë™ì¼í•©ë‹ˆë‹¤.

ì¶œì²˜ = ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators - https://openreview.net/pdf?id=r1xMH1BtvB

ì¶œì²˜ = KoELECTRA Github - https://github.com/monologg/KoELECTRA/tree/master

ì¶œì²˜ = koelectra-base-v3-discriminator Huggingface - https://huggingface.co/monologg/koelectra-base-v3-discriminator

---
### 3ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ Fine-Tunning & Learning Process

python 3.7.16 

transformers==2.1.1

pytorch==1.13.1

RTX A4000 / CUDA 10.0

**AdamW** ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í–ˆìœ¼ë©°, í•™ìŠµì—ì„œì˜ ì •ì²´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ **lr**(Learning Rate)ì„ **2e-4**(KoELECTRA) ì—ì„œ **5e-6**ìœ¼ë¡œ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.(ë‚˜ì¤‘ì— REDUCELRONPLATEAU í•¨ìˆ˜ ì ìš©í•´ì„œ ë³€ê²½í•´ì•¼ í•¨)

ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¦¬ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ê°ê° ê¸ì • ë˜ëŠ” ë¶€ì • ë˜ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ë¼ë²¨ë§ëœ í•œêµ­ì–´ corpus ì•½ 59ë§Œê°œë¥¼ ì¶”ê°€í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.(í‰ì„œë¬¸, ëŒ€í™”ì²´, snsì²´ ë“±)

íš¨ìœ¨ì ì¸ padding ê¸¸ì´ë¥¼ ì‚°ì •í•  ìˆ˜ ìˆë„ë¡, **./learning/__init__.py** íŒŒì¼ì— **dataset_split** í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.

overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ dropoutì€ ì²˜ìŒë¶€í„° **0.2**ë¡œ ì„¤ì •ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

**koelectra-base-v3-discriminator** ëª¨ë¸ì€ **ElectraForSequenceClassification** ëª¨ë¸ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. 
**ElectraForSequenceClassification**ëŠ” Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ **ELECTRA** ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¬¸ì¥ ë¶„ë¥˜(Classification) ëª¨ë¸ì…ë‹ˆë‹¤. **ElectraForSequenceClassification**ëª¨ë¸ì€ ELECTRA ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ê³ , ê·¸ ë‹¤ìŒì— íŠ¹ì • ë¶„ë¥˜ ì‘ì—…ì— ë§ê²Œ ì¶”ê°€ë¡œ í•™ìŠµë˜ëŠ” ë ˆì´ì–´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë ˆì´ì–´ëŠ” ì£¼ì–´ì§„ ì…ë ¥ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

---
### ëª¨ë¸ êµ¬ì¡°

1. **ElectraModel**: Electraì˜ ê¸°ë³¸ Transformer ëª¨ë¸ìœ¼ë¡œ ì£¼ì–´ì§„ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    - ElectraEmbeddings: í† í°ë“¤ì„ ì„ë² ë”©í•˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ, í† í°, ìœ„ì¹˜, segment(ë¬¸ì¥ì˜ êµ¬ë¶„) ì„ë² ë”©ì„ ê²°í•©í•˜ì—¬ í† í°ì˜ ì˜ë¯¸ì ì¸ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ElectraEncoder: ì—¬ëŸ¬ ê°œì˜ ElectraLayerìŒ“ì€ í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì¸ì½”ë”©ê³¼ ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.
    - ElectraLayer: ê° ë ˆì´ì–´ëŠ” Multi-Head Self Attentionê³¼ Feed-Forward Neural Networkë“±ì„ í†µí•´ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ì´í•´í•¨.
       
2. **ElectraClassificationHead**: ì´ ë¶€ë¶„ì€ ElectraModelì˜ ì¶œë ¥ì„ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - dense: ì…ë ¥ ì„ë² ë”©ì„ 768ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” fully connected ì„ í˜• ë ˆì´ì–´ì…ë‹ˆë‹¤
    - out_proj: 768ì°¨ì›ì˜ ë²¡í„°ë¥¼ 3ê°œì˜ í´ë˜ìŠ¤(ê¸ì •, ë¶€ì •, ì¤‘ë¦½)ìœ¼ë¡œ mappingí•˜ëŠ” ì„ í˜• ë ˆì´ì–´ì…ë‹ˆë‹¤

ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì€, ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì‹œí€€ìŠ¤ë¥¼ 3ê°œì˜ í´ë˜ìŠ¤ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì—¬ í•´ë‹¹ í…ìŠ¤íŠ¸ê°€ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

---
### ì‹œê°í™” íŒŒì¼(ì˜ˆì‹œ)

**dataset_split** í•¨ìˆ˜ ì‹¤í–‰ ì‹œ, sentenceë¥¼ tokenizing í•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ë¹„ìœ¨ì„ ì„¤ì •í•˜ì—¬ ìµœì ì˜ **padding_length**ë¥¼ ë„ì¶œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (**Plotly**)
![Padding_length visualization](./visualization/padding_length.PNG)

**plot_training_progress** í•¨ìˆ˜ ì‹¤í–‰ ì‹œ, **train_with_early_stopping**í•¨ìˆ˜ë¥¼ í†µí•œ ëª¨ë¸ì˜ í•™ìŠµê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. (**Plotly**)
![Learing process visualization](./visualization/plot_vis.PNG)
